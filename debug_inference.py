# debug_inference.py
# è¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºæµ‹è¯•å’Œè°ƒè¯•æ¨ç†æµç¨‹çš„è„šæœ¬ã€‚

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
from peft import PeftModel
import os

# --- ä½¿ç”¨å’Œmain.pyå®Œå…¨ç›¸åŒçš„å¸¸é‡ ---
BASE_MODEL_ID = "Qwen/Qwen1.5-1.8B-Chat"
OUTPUT_DIR = "output/puns-generator-checkpoint"

print("--- å¼€å§‹æ‰§è¡Œæ¨ç†è°ƒè¯•è„šæœ¬ ---")

try:
    # --- 1. åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨ ---
    print(f"--> æ­¥éª¤1: æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹ '{BASE_MODEL_ID}'...")
    # æˆ‘ä»¬åœ¨CPUä¸ŠåŠ è½½ä»¥è¿›è¡Œæµ‹è¯•
    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, device_map="cpu")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)
    print("--> åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸï¼")

    # --- 2. åŠ è½½LoRAé€‚é…å™¨ ---
    print(f"--> æ­¥éª¤2: æ­£åœ¨åŠ è½½LoRAé€‚é…å™¨ '{OUTPUT_DIR}'...")
    # PeftModel ä¼šå°†LoRAæƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­
    model = PeftModel.from_pretrained(model, OUTPUT_DIR)
    print("--> LoRAé€‚é…å™¨åŠ è½½æˆåŠŸï¼æ¨¡å‹å·²å‡†å¤‡å°±ç»ªã€‚")

    # --- 3. å‡†å¤‡è¾“å…¥ ---
    print("--> æ­¥éª¤3: æ­£åœ¨å‡†å¤‡æµ‹è¯•è¾“å…¥...")
    prompt = "è®²ä¸ªå…³äºç¨‹åºå‘˜çš„ç¬‘è¯"
    messages = [{"role": "user", "content": prompt}]
    
    # ä½¿ç”¨åˆ†è¯å™¨çš„èŠå¤©æ¨¡æ¿æ¥æ ¼å¼åŒ–è¾“å…¥
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt")
    print(f"--> å‡†å¤‡å¥½çš„è¾“å…¥æ–‡æœ¬: {text}")

    # --- 4. æ‰§è¡Œç”Ÿæˆ (æœ€å…³é”®çš„æµ‹è¯•æ­¥éª¤) ---
    print("--> æ­¥éª¤4: æ­£åœ¨è°ƒç”¨ model.generate() ...")
    
    # ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰ç¡®å®šçš„ç¨³å®šç”Ÿæˆé…ç½®
    generation_config = GenerationConfig(
        max_new_tokens=100,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.2,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id,
    )

    generated_ids = model.generate(
        model_inputs.input_ids,
        generation_config=generation_config
    )
    print("--> model.generate() è°ƒç”¨æˆåŠŸï¼")

    # --- 5. è§£ç è¾“å‡º ---
    print("--> æ­¥éª¤5: æ­£åœ¨è§£ç è¾“å‡º...")
    decoded_output = tokenizer.batch_decode(generated_ids[:, model_inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]

    print("\n===================================")
    print("      ğŸ‰ è°ƒè¯•æˆåŠŸï¼ğŸ‰")
    print("===================================")
    print(f"æ¨¡å‹ç”Ÿæˆçš„å›ç­”æ˜¯: {decoded_output}")

except Exception as e:
    print("\n===================================")
    print("      ğŸ”¥ è°ƒè¯•å¤±è´¥ï¼ğŸ”¥")
    print("===================================")
    print(f"åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­é‡åˆ°äº†é”™è¯¯: {e}")
    import traceback
    traceback.print_exc()